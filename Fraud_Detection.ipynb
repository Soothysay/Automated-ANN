{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fraud Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1-eo-1RZansiJhUpavoN5MGgzYY9kpEF7",
      "authorship_tag": "ABX9TyPCT2F53WKxlqik/UMbSEOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soothysay/Automated-ANN/blob/master/Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wBiRWrNtmhU",
        "colab_type": "code",
        "outputId": "345d9eb2-ab40-4a18-ce80-334516a01991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import utils\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten,Dense,Activation,Embedding,LeakyReLU,BatchNormalization,Dropout\n",
        "from keras.activations import relu,sigmoid,elu,softmax,tanh,softplus\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import datasets\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import History\n",
        "from keras import losses\n",
        "from keras import optimizers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzvVroWoubov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read():\n",
        "  df=pd.read_csv('/content/drive/My Drive/ANN/PS_20174392719_1491204439457_log.csv')\n",
        "  # list of columns whose data type is object i.e. string\n",
        "  listOfColumnNames = list((df.dtypes[df.dtypes == np.object]).index)\n",
        "  #filteredColumns = df.dtypes[df.dtypes == np.object]\n",
        "  #listOfColumnNames = list(filteredColumns.index)\n",
        "  #Removing the Name and Surname Columns (You can add more redundant parameters)\n",
        "  if 'nameOrig' in listOfColumnNames:\n",
        "    df=df.drop(['nameOrig'],axis=1)\n",
        "  if 'nameDest' in listOfColumnNames:\n",
        "    df=df.drop(['nameDest'],axis=1)\n",
        "  if 'nameOrig' in listOfColumnNames:\n",
        "    listOfColumnNames.remove('nameOrig')\n",
        "  if 'nameDest' in listOfColumnNames:\n",
        "    listOfColumnNames.remove('nameDest')\n",
        "  #Replacing the object type values with classified values (Performance can be improved)\n",
        "  for i in range(len(listOfColumnNames)):\n",
        "    s=set(df[listOfColumnNames[i]])\n",
        "    s=list(s)\n",
        "    for j in range(len(s)):\n",
        "      df=df.replace(to_replace=s[j],value=(j+1))\n",
        "  #Done with data shaping\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2QKSfHctyIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def divide(path,divider):\n",
        "  #Division of X and Y\n",
        "  x=path.drop(divider,axis=1)\n",
        "  type(divider)\n",
        "  y=path[divider]\n",
        "  return x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKVY1h3guTt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "def splitter(x,y):\n",
        "  \n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "  # Feature Scaling\n",
        "  sc = StandardScaler()\n",
        "  x_train = sc.fit_transform(x_train)\n",
        "  x_test = sc.transform(x_test)\n",
        "  return x_train,y_train,x_test,y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9BoXSsf6Mfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inputter(df):\n",
        "  print(\"Enter the column you wish to use as  result\")\n",
        "  print(\"Your options are:\",df.columns)\n",
        "  divider=input()\n",
        "  #We need to preprocess data a bit more if data is biased towards a particular outcome\n",
        "  nonf=df[df[divider]==0]\n",
        "  fra=df[df[divider]==1]\n",
        "  #I have just taken 10. The value can change\n",
        "  if nonf.shape>=(10*fra.shape): \n",
        "    nonf=nonf.sample(2*fra.shape[0])\n",
        "    #print(nonf.shape)\n",
        "    df=fra.append(nonf,ignore_index=True)\n",
        "  if fra.shape>=(10*nonf.shape):\n",
        "    fra=fra.sample(2*nonf.shape[0])\n",
        "    #print(fra.shape)\n",
        "    df=nonf.append(fra,ignore_index=True)\n",
        "  return df,divider"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYojV5gBbvc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotter(model):\n",
        "  #Summarize history for accuracy\n",
        "  plt.plot(model.history['accuracy'])\n",
        "  plt.plot(model.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  #Summarize history for loss\n",
        "  plt.plot(model.history['loss'])\n",
        "  plt.plot(model.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQlFZa81aiIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(activation='relu',optimizer='adam',init_mode='uniform',dropout_rate=0.0,n=10,p=10,neuron=12,learning_rate=0.002):\n",
        "  #Creating the Artificial Neural Network using Keras for hyperparameter tuning\n",
        "  model=Sequential()\n",
        "  #Input Layer\n",
        "  model.add(Dense(neuron,input_dim=p,kernel_initializer=init_mode, activation=activation))\n",
        "  #Adding hidden layers\n",
        "  for i in range(2,n):\n",
        "    model.add(Dense(neuron,kernel_initializer=init_mode,activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "  #Adding output layer\n",
        "  model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "  #As output will have 2 values, so using binary crossentropy\n",
        "  if(optimizer=='SGD'):\n",
        "    opti=keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  if(optimizer=='Adam'):\n",
        "    opti=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  if(optimizer=='Adamax'):\n",
        "    opti=keras.optimizers.Adamax(learning_rate=learning_rate)\n",
        "  if(optimizer=='Nadam'):\n",
        "    opti=keras.optimizers.Nadam(learning_rate=learning_rate)\n",
        "  if(optimizer=='Adagrad'):\n",
        "    opti=keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
        "  if(optimizer=='RMSprop'):\n",
        "    opti=keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
        "  return(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQGb7MYtOne_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tuner(p,x_test,y_test,epochs,batch_size):\n",
        "  model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "  # Define the grid search parameters\n",
        "  activation = ['softmax', 'relu', 'tanh', 'sigmoid','elu','softplus']\n",
        "  optimizer = ['SGD', 'Adam', 'Adamax', 'Nadam','Adagrad','RMSprop']\n",
        "  init_mode = ['glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "  # User Automated hyperparameters\n",
        "  n=list()\n",
        "  dropout_rate=list()\n",
        "  neuron=list()\n",
        "  learning_rate=list()\n",
        "  while(True):\n",
        "    print(\"Input n:\")\n",
        "    n.append(int(input()))\n",
        "    print(\"Enter number of neurons in each layer\")\n",
        "    neuron.append(int(input()))\n",
        "    co=input(\"Continue?(Y/N)\")\n",
        "    if(co=='N'):\n",
        "      break\n",
        "  while(True):\n",
        "    print(\"Input Dropout percentage:\")\n",
        "    dropout_rate.append((float(input())*0.01))\n",
        "    co=input(\"Continue?(Y/N)\")\n",
        "    if(co=='N'):\n",
        "      break\n",
        "  #Put learning_rate\n",
        "  while(True):\n",
        "    print(\"Input Learning rate:\")\n",
        "    learning_rate.append(float(input()))\n",
        "    co=input(\"Continue?(Y/N)\")\n",
        "    if(co=='N'):\n",
        "      break\n",
        "  #Creation of variable parameters\n",
        "  param_grid = dict(activation=activation,optimizer=optimizer,init_mode=init_mode,dropout_rate=dropout_rate,n=n,p=p,neuron=neuron,learning_rate=learning_rate)\n",
        "  #Invoking Randomized search with cross validation value 3 (Baad mein mai ne user se hi puch liya)\n",
        "  cv=int(input(\"Enter the value of the number of folds you want:\"))\n",
        "  grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,n_jobs=-1,cv=cv)\n",
        "  #grid=GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,cv=cv)\n",
        "  grid_result = grid.fit(x_test,y_test)\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "  cv_results_df = pd.DataFrame(grid_result.cv_results_)\n",
        "  cv_results_df.to_csv('gridsearch.csv')\n",
        "  print(cv_results_df)\n",
        "  #plot_grid_search(grid_result.cv_results_, activation, optimizer, 'Activation Function', 'Optimizer')\n",
        "      #Add GridsearchCV\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02ryXgswaty5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
        "    # Get Test Scores Mean and std for each grid search\n",
        "    scores_mean = cv_results['mean_test_score']\n",
        "    #scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
        "\n",
        "    scores_sd = cv_results['std_test_score']\n",
        "    #scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
        "\n",
        "    # Plot Grid search scores\n",
        "    _, ax = plt.subplots(1,1)\n",
        "\n",
        "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
        "    for idx, val in enumerate(grid_param_2):\n",
        "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
        "\n",
        "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
        "    ax.set_xlabel(name_param_1, fontsize=16)\n",
        "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
        "    ax.legend(loc=\"best\", fontsize=15)\n",
        "    ax.grid('on')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEEr0If4iyEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossmat(m,x_test,y_test):\n",
        "  y_pred = m.predict(x_test)\n",
        "  y_pred = (y_pred > 0.5)\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  #Creating confusion matrices\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  #As binary classification\n",
        "  TP = cm[1, 1]\n",
        "  TN = cm[0, 0]\n",
        "  FP = cm[0, 1]\n",
        "  FN = cm[1, 0]\n",
        "  a=cm.shape\n",
        "  #Calculating false positives\n",
        "  corrPred=0\n",
        "  falsePred=0\n",
        "  for row in range(a[0]):\n",
        "    for c in range(a[1]):\n",
        "      if row == c:\n",
        "        corrPred +=cm[row,c]\n",
        "      else:\n",
        "        falsePred += cm[row,c]\n",
        "  print('Correct predictions: ', corrPred)\n",
        "  print('False predictions', falsePred)\n",
        "  # Calculate the Accuracy\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  score=accuracy_score(y_pred,y_test)\n",
        "  #Calculate sensitivity\n",
        "  from sklearn.metrics import recall_score\n",
        "  sensitivity=recall_score(y_pred,y_test)\n",
        "  #Calculate precision\n",
        "  from sklearn.metrics import precision_score\n",
        "  precision=precision_score(y_pred,y_test)\n",
        "  #Calculate specificity\n",
        "  specificity = TN / (TN + FP)\n",
        "  #Calculate Cohen's Kappa Score\n",
        "  from sklearn.metrics import cohen_kappa_score\n",
        "  kappa=cohen_kappa_score(y_pred,y_test)\n",
        "  #Showing classification Report \n",
        "  from sklearn.metrics import classification_report\n",
        "  target_names = ['class 0', 'class 1']\n",
        "  print(classification_report(y_pred, y_test, target_names=target_names))\n",
        "\n",
        "  return cm,score,sensitivity,precision,specificity,kappa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUZw_BdvvYS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_class(x_train,nodes,activation,n):\n",
        "  #Creating customized ANN Model\n",
        "  model=Sequential()\n",
        "  for i in range(len(nodes)):\n",
        "    if(i==0):\n",
        "      if(activation=='sigmoid'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'glorot_uniform',activation='sigmoid',input_dim = len(x_train[1])))\n",
        "      if(activation=='relu'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'he_uniform',activation='relu',input_dim = len(x_train[1])))\n",
        "      if(activation=='tanh'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'glorot_normal',activation='tanh',input_dim = len(x_train[1])))\n",
        "      if(activation=='softmax'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'glorot_normal',activation='softmax',input_dim = len(x_train[1])))\n",
        "      if(activation== 'elu'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'he_normal',activation='elu',input_dim = len(x_train[1])))\n",
        "      if(activation=='softplus'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'he_normal',activation='softplus',input_dim = len(x_train[1])))\n",
        "    else:\n",
        "      if(activation=='sigmoid'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'glorot_uniform',activation='sigmoid'))\n",
        "      if(activation=='relu'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'he_uniform',activation='relu'))\n",
        "      if(activation=='tanh'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'glorot_normal',activation='tanh'))\n",
        "      if(activation=='softmax'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'glorot_uniform',activation='softmax'))\n",
        "      if(activation=='elu'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'he_normal',activation='elu'))\n",
        "      if(activation=='softplus'):\n",
        "        model.add(Dense(units = nodes[i], kernel_initializer = 'he_normal',activation='softplus'))\n",
        "    model.add(Dropout(n))\n",
        "  #Adding output layer\n",
        "  model.add(Dense(units=1, kernel_initializer = 'glorot_uniform',activation='sigmoid'))\n",
        "  #model.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKuNGdFQugVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optibin(model,opt,x_train,y_train,spl,bs,epochs,x_test,y_test):\n",
        "  #Choosing the proper optimizer to use\n",
        "  if(opt=='sgd'):\n",
        "    print(\"Enter Momentum:\")\n",
        "    mom=float(input())\n",
        "    lr=float(input(\"Enter value of Learning rate:\"))\n",
        "    opti=keras.optimizers.SGD(learning_rate=lr, momentum=mom, nesterov=False)\n",
        "  if(opt=='Adam'):\n",
        "    lr=float(input(\"Enter value of Learning rate:\"))\n",
        "    opti=keras.optimizers.Adam(learning_rate=lr)\n",
        "  if(opt=='Adamax'):\n",
        "    lr=float(input(\"Enter value of Learning rate:\"))\n",
        "    beta_1=float(input(\"Enter value of beta 1 (Generally close to 1)\"))\n",
        "    beta_2=float(input(\"Enter value of beta 2 (Generally close to 1)\"))\n",
        "    opti=keras.optimizers.Adamax(learning_rate=lr, beta_1=beta_1, beta_2=beta_2)\n",
        "  if(opt=='Nadam'):\n",
        "    lr=float(input(\"Enter value of Learning rate:\"))\n",
        "    beta_1=float(input(\"Enter value of beta 1 (Generally close to 1)\"))\n",
        "    beta_2=float(input(\"Enter value of beta 2 (Generally close to 1)\"))\n",
        "    opti=keras.optimizers.Nadam(learning_rate=lr, beta_1=beta_1, beta_2=beta_2)\n",
        "  if(opt=='RMSprop'):\n",
        "    lr=float(input(\"Enter value of Learning rate:\"))\n",
        "    opti=keras.optimizers.RMSprop(learning_rate=lr)\n",
        "  if(opt=='Adagrad'):\n",
        "    lr=float(input(\"Enter value of Learning rate:\"))\n",
        "    opti=keras.optimizers.Adagrad(learning_rate=lr)\n",
        "  model.compile(optimizer = opti, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "  model_history=model.fit(x_train, y_train,validation_split=spl, batch_size = bs,epochs = epochs)\n",
        "  return model_history, model\n",
        " # model.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61Jv8BRZu2T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  seed = 7\n",
        "  np.random.seed(seed)\n",
        "  df=read()\n",
        "  while True:\n",
        "    print(\"Ready to work?(Y/N)\")\n",
        "    work=input()\n",
        "    if(work=='N'):\n",
        "      break\n",
        "    df,divider=inputter(df)\n",
        "    #Dividing dataset to x and y\n",
        "    x,y=divide(df,divider)\n",
        "    #Scaling the dataset and division of training and test dataset\n",
        "    x_train,y_train,x_test,y_test=splitter(x,y)\n",
        "    print(\"Choose appropriate Option:\")\n",
        "    print(\"1. Tune to determine best model using hyperparameter tuning\")\n",
        "    print(\"2. Train and test your custom model\")\n",
        "    option=input()\n",
        "    if(option=='1'):\n",
        "      #Hyperparameter tuning\n",
        "      p=list()\n",
        "      p.append(len(x_test[1]))\n",
        "      print(\"Enter number of epochs (Try keeping between 40-100):\")\n",
        "      epochs=int(input())\n",
        "      print(\"Enter batch size (between 10-20):\")\n",
        "      batch_size=int(input())\n",
        "      # create function for tuning\n",
        "      tuner(p,x_test,y_test,epochs,batch_size)\n",
        "    if(option=='2'):\n",
        "      print(\"Number of layers you want\") #Between 2 to 9\n",
        "      nl=int(input())\n",
        "      nodes=list()\n",
        "      for i in range(nl):\n",
        "        nodes.append(int(input(\"Enter number of neurons in layer: \")))\n",
        "      epochs=int(input(\"Enter the number of epochs you want\"))\n",
        "      bs=int(input(\"Enter the batch size you want\"))\n",
        "      spl=int(input(\"Enter the percentage of data you want to use for validation\"))\n",
        "      spl=spl*0.01\n",
        "      print(\"Choose the activation function for your layers\")\n",
        "      print(\"1. sigmoid\")\n",
        "      print(\"2. relu (Recommended. Most worked activation function in industry)\")\n",
        "      print(\"3. tanh\")\n",
        "      print(\"4. softmax\")\n",
        "      print(\"5. elu\")\n",
        "      print(\"6. softplus\")\n",
        "      activation=input()\n",
        "      n=int(input(\"Enter percentage of dropout\"))\n",
        "      n=n*0.01\n",
        "      model=binary_class(x_train,nodes,activation,n)\n",
        "      print(\"Enter optimizer:\")\n",
        "      print(\"1. sgd\")\n",
        "      print(\"2. Adam (Hyperparameter tuning shows generally best)\")\n",
        "      print( \"3. Adamax\")\n",
        "      print(\"4. Nadam\")\n",
        "      print(\"5. Adagrad\")\n",
        "      print(\"6. RMSprop\")\n",
        "      opt=input()\n",
        "      #model_history=model.fit(x_train, y_train,validation_split=spl, batch_size = bs,epochs = epochs)\n",
        "      m1,m=optibin(model,opt,x_train,y_train,spl,bs,epochs,x_test,y_test)\n",
        "      #from ann_visualizer.visualize import ann_viz;\n",
        "      #ann_viz(m1, title=\"My first neural network\")\n",
        "      print(m1.history.keys())\n",
        "      plotter(m1)\n",
        "\n",
        "      cm,score,sensitivity,precision,specificity,kappa=crossmat(m,x_test,y_test)\n",
        "      print(\"Confusion Matrix=\",cm)\n",
        "      print(\"Accuracy Score=\",score)\n",
        "      print(\"Sensitivity Score=\",sensitivity)\n",
        "      print(\"Precision Score=\",precision)\n",
        "      print(\"Specificity Score=\",specificity)\n",
        "      print(\"Cohen's Kappa Score=\",kappa)\n",
        "  print(\"Thank You!\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvvRRQ6DOnCN",
        "colab_type": "code",
        "outputId": "7717e4f2-116d-4e54-eff8-f3e250e6f1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ready to work?(Y/N)\n",
            "Y\n",
            "Enter the column you wish to use as  result\n",
            "Your options are: Index(['step', 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
            "       'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud'],\n",
            "      dtype='object')\n",
            "isFraud\n",
            "Choose appropriate Option:\n",
            "1. Tune to determine best model using hyperparameter tuning\n",
            "2. Train and test your custom model\n",
            "1\n",
            "Enter number of epochs (Try keeping between 40-100):\n",
            "40\n",
            "Enter batch size (between 10-20):\n",
            "10\n",
            "Input n:\n",
            "4\n",
            "Enter number of neurons in each layer\n",
            "12\n",
            "Continue?(Y/N)Y\n",
            "Input n:\n",
            "5\n",
            "Enter number of neurons in each layer\n",
            "10\n",
            "Continue?(Y/N)N\n",
            "Input Dropout percentage:\n",
            "10\n",
            "Continue?(Y/N)N\n",
            "Input Learning rate:\n",
            "0.001\n",
            "Continue?(Y/N)Y\n",
            "Input Learning rate:\n",
            "0.002\n",
            "Continue?(Y/N)N\n",
            "Enter the value of the number of folds you want:3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best: 0.966518 using {'p': 8, 'optimizer': 'Nadam', 'neuron': 10, 'n': 5, 'learning_rate': 0.001, 'init_mode': 'he_uniform', 'dropout_rate': 0.1, 'activation': 'tanh'}\n",
            "0.693385 (0.014848) with: {'p': 8, 'optimizer': 'SGD', 'neuron': 12, 'n': 4, 'learning_rate': 0.001, 'init_mode': 'glorot_uniform', 'dropout_rate': 0.1, 'activation': 'relu'}\n",
            "0.883519 (0.012010) with: {'p': 8, 'optimizer': 'RMSprop', 'neuron': 12, 'n': 5, 'learning_rate': 0.001, 'init_mode': 'glorot_normal', 'dropout_rate': 0.1, 'activation': 'sigmoid'}\n",
            "0.784092 (0.014923) with: {'p': 8, 'optimizer': 'Adagrad', 'neuron': 10, 'n': 4, 'learning_rate': 0.002, 'init_mode': 'he_uniform', 'dropout_rate': 0.1, 'activation': 'softplus'}\n",
            "0.910103 (0.009472) with: {'p': 8, 'optimizer': 'Adam', 'neuron': 10, 'n': 4, 'learning_rate': 0.001, 'init_mode': 'glorot_uniform', 'dropout_rate': 0.1, 'activation': 'softplus'}\n",
            "0.928977 (0.001611) with: {'p': 8, 'optimizer': 'Adam', 'neuron': 12, 'n': 5, 'learning_rate': 0.002, 'init_mode': 'he_normal', 'dropout_rate': 0.1, 'activation': 'softmax'}\n",
            "0.856935 (0.017628) with: {'p': 8, 'optimizer': 'Adagrad', 'neuron': 12, 'n': 5, 'learning_rate': 0.002, 'init_mode': 'glorot_normal', 'dropout_rate': 0.1, 'activation': 'tanh'}\n",
            "0.966518 (0.004553) with: {'p': 8, 'optimizer': 'Nadam', 'neuron': 10, 'n': 5, 'learning_rate': 0.001, 'init_mode': 'he_uniform', 'dropout_rate': 0.1, 'activation': 'tanh'}\n",
            "0.916595 (0.015258) with: {'p': 8, 'optimizer': 'Nadam', 'neuron': 12, 'n': 4, 'learning_rate': 0.001, 'init_mode': 'glorot_uniform', 'dropout_rate': 0.1, 'activation': 'softmax'}\n",
            "0.666803 (0.007671) with: {'p': 8, 'optimizer': 'Adagrad', 'neuron': 10, 'n': 5, 'learning_rate': 0.002, 'init_mode': 'glorot_uniform', 'dropout_rate': 0.1, 'activation': 'softmax'}\n",
            "0.666803 (0.007671) with: {'p': 8, 'optimizer': 'SGD', 'neuron': 12, 'n': 5, 'learning_rate': 0.001, 'init_mode': 'he_uniform', 'dropout_rate': 0.1, 'activation': 'sigmoid'}\n",
            "   mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
            "0      59.497266      4.358023  ...        0.014848                8\n",
            "1      61.551273      0.290596  ...        0.012010                5\n",
            "2      52.496391      0.197614  ...        0.014923                7\n",
            "3      64.005209      0.452244  ...        0.009472                4\n",
            "4      70.438691      0.066167  ...        0.001611                2\n",
            "5      57.315865      0.178268  ...        0.017628                6\n",
            "6      78.757628      0.398541  ...        0.004553                1\n",
            "7      73.215059      0.358421  ...        0.015258                3\n",
            "8      61.383119      0.886433  ...        0.007671                9\n",
            "9      56.783863      0.454465  ...        0.007671                9\n",
            "\n",
            "[10 rows x 19 columns]\n",
            "Ready to work?(Y/N)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-3ff7f994ef89>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ready to work?(Y/N)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}